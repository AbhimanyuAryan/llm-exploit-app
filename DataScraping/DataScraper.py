# from DBController import DBController

import requests as req
import json

from dotenv import load_dotenv
import os

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from time import sleep
import regex as re


load_dotenv()

class DataScraper:
    def __init__(self):
        self.scrape()

    
    def scrape(self):
        print("Choose a source to scrape data from:")
        print("1. Scrape from CVE website")
        print("2. Scrape from NVD API")
        print("3. Scrape from stackoverflow API")
        choice = input("Choose an option: ")
        if choice == "1":
            self.scrape_cve()
        elif choice == "2":
            self.scrape_nvd()
        elif choice == "3":
            self.scrapeStackOverflow()
        else:
            print("Invalid choice. Please try again.")
    
    def scrape_cve(self, limit=200):
        # Prepare a folder to store the jsons
        if not os.path.exists("cve_vulns"):
            os.mkdir("cve_vulns")

        try:
            # Initialize the Chrome WebDriver
            driver = webdriver.Chrome()  # specify the path to chromedriver

            # Navigate to the CVE website   
            driver.get("https://www.exploit-db.com/")

            # Wait for the page to load
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "exploits-table")))

            sleep(1)
            # Find the table element containing vulnerability information
            mainTable = driver.find_element(By.ID, "exploits-table")
            exploitsTable = mainTable.find_element(By.TAG_NAME, "tbody")
        
            #Lets get the number of total exploits in the page
            totalExploits = driver.find_element(By.CLASS_NAME, "dataTables_info")
            totalExploits = re.findall(r'Showing \d+ to \d+ of (.*) entries', totalExploits.text)[0]
            #Lets convert the number of exploits to an integer, but it has commas so lets replace them
            totalExploits = int(totalExploits.replace(",", ""))
            
            exploitsScraped = 0
            #Lets open the first exploit in the table and extract its link 
            exploit = exploitsTable.find_element(By.TAG_NAME, "tr")
            row = exploit.find_elements(By.TAG_NAME, "td")

            #Lets get the link to the exploit and follow it
            linkForVuln = row[4].find_element(By.TAG_NAME, "a").get_attribute("href")
            #The driver can change to the new tab since we wont come back to the main page to scrape the rest of the data
            while(exploitsScraped < totalExploits and exploitsScraped < limit):
                #TODO: Verify if the exploit name is already in the database if so stop scraping

                driver.get(linkForVuln)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "code")))

                #We now have everything we need to scrape the exploit
                sleep(1)
                source = linkForVuln
                vulnName = driver.find_element(By.TAG_NAME, "h1").text
                stats = driver.find_elements(By.CLASS_NAME, "card-stats")
                #The card stats[0] contains data relative to the exploit id
                exploitData = stats[0].find_elements(By.TAG_NAME, "h6")
                vulnEDBID = exploitData[0].text
                vulnCVE = exploitData[1].text

                #The card stats[1] contains data relative to the author name and the type of exploit
                exploitData = stats[1].find_elements(By.TAG_NAME, "h6")
                author = exploitData[0].text
                exploitType = exploitData[1].text

                #The card stats[2] contains data relative to the platform and the date the exploit was published
                exploitData = stats[2].find_elements(By.TAG_NAME, "h6")
                platform = exploitData[0].text
                datePublished = exploitData[1].text

                vulnContent = driver.find_element(By.TAG_NAME, "code").text

                # embedding = self.generate_embedding(vulnContent)
                #Lets save the exploit to a file
                payload = {
                    "source": source,
                    "vulnName": vulnName,
                    "vulnEDBID": vulnEDBID,
                    "vulnCVE": vulnCVE,
                    "author": author,
                    "exploitType": exploitType,
                    "platform": platform,
                    "datePublished": datePublished,
                    "vulnContent": vulnContent
                }


                # data = [payload]
                #Lets insert the new exploit into the database
                # self.db.insert_vulnerability(data)

                
                nameToSave = vulnName.replace(" ", "_")
                nameToSave = nameToSave.replace("/", "_")
                with open(f"cve_vulns/{nameToSave}.json", "w") as f:
                    f.write(json.dumps(payload))
                    print(f"Exploit {vulnName} saved to file")
                
                #Go to the previous exploit page
                #Lets get the link to the exploit by locating the previous arrow button
                prevButtonDiv = driver.find_element(By.CLASS_NAME, "ml-3")
                prevButton = prevButtonDiv.find_element(By.TAG_NAME, "a")
                link = prevButton.get_attribute("href")
                linkForVuln = link
                exploitsScraped += 1

        except Exception as e:
            print("An error occurred while scraping:", str(e))


    def scrape_nvd(self):
        # API_KEY = os.getenv("NVD_API_KEY")

        # Prepare a folder to store the jsons
        if not os.path.exists("nvd_vulns"):
            os.mkdir("nvd_vulns")


        # NVD API endpoint for retrieving CVE details
        startIndex = 0
        resultsPerPage = 1
        maxResults = 1
        nvd_api_url = f"https://services.nvd.nist.gov/rest/json/cves/2.0/?resultsPerPage={resultsPerPage}&startIndex={startIndex}"
        #Lets get the number of total exploits in the page
        response = req.get(nvd_api_url)
        data = response.json()
        totalExploits = data["totalResults"]
        #Set to 50 since is the maximum number of results per page the API allows for each 5 seconds 
        resultsPerPage = 50

        print(totalExploits)
        for _ in range(totalExploits // resultsPerPage):
            try:
                # Send a request to the NVD API
                response = req.get(nvd_api_url)
                data = response.json()
                # Check if the response contains CVE details
                if response.status_code == 200:
                    #Lets extract the vulnerabilities from the data
                    vulnerabilities = data["vulnerabilities"]
                    for vuln in vulnerabilities:
                        payload = {
                            "CVE_ID" : vuln["cve"]["id"],
                            "source" : vuln["cve"]["sourceIdentifier"],
                            "lastModifiedDate" : vuln["cve"]["lastModifiedDate"],
                        }


                # Update the start index for the next request
                startIndex += resultsPerPage
                nvd_api_url = f"https://services.nvd.nist.gov/rest/json/cves/2.0/?resultsPerPage={resultsPerPage}&startIndex={startIndex}"
            except Exception as e:
                print("An error occurred while scraping:", str(e))
                break


    def scrapeStackOverflow(self, tags=None):
        if not os.path.exists("stackoverflow_qna"):
            os.mkdir("stackoverflow_qna")

        pageNum = 1
        pageSize = 100
        authKey = "U4DMV*8nvpm3EOpvf69Rxw(("
        apiEndpoint = f"https://api.stackexchange.com/2.3/"
        stackFilter = "!51HTsOQRBj9p0_DazD)W)-DUaqtDn88)Rxjmvq"

        for _ in range(5):
            try:
               #Send request to the api
                link = f"{apiEndpoint}questions?page={pageNum}&pagesize={pageSize}&order=desc&sort=activity&key={authKey}&filter={stackFilter}&site=stackoverflow"
                response = req.get(link)
                if response.status_code==200:
                    
                    data = response.json()
                    #Lets extract the questions from the data
                    questions = data["items"]
                    for q in questions:
                        if q["is_answered"]:

                            questionTitle = q["title"]
                            questionBody = q["body"]
                            
                            #Lets try and get the accepted answer
                            try:
                                if q["is_accepted"]:
                                    
                                    acceptedAnswerId = q["accepted_answer_id"]
                                    for answer in q["answers"]:
                                        if answer["answer_id"] == acceptedAnswerId:
                                            acceptedAnswer = answer["body"]

                            except Exception as e:
                                #Lets get the one with the most upvotes if there is no answer then we will skip this question
                                acceptedAnswer = None
                                bestScore = 0
                                responses = q["answers"]
                                for r in responses:
                                    if r["score"] >= bestScore:
                                        acceptedAnswer = r["body"]
                                        bestScore = r["score"]
                            
                            if acceptedAnswer is not None:
                                payload = {
                                    "questionTitle": questionTitle,
                                    "question": questionBody,
                                    "acceptedAnswer": acceptedAnswer,
                                    "questionSource": q["link"]
                                }
                                #Lets insert into a json file
                                questionID = q["question_id"]
                                with open(f"stackoverflow_qna/stackoverflow_{questionID}.json", "w") as f:
                                    json.dump(payload, f)
                            
                pageNum+=1

            except Exception as e:
                print("An error occurred while scraping:", str(e))
                break
        
        print("Finished scraping stackoverflow")

    
                    
DataScraper()