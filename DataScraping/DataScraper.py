# from DBController import DBController

import requests as req
import json

from dotenv import load_dotenv
import os

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from time import sleep
import regex as re


load_dotenv()

class DataScraper:
    def __init__(self):
        self.scrape()

    
    def scrape(self):
        print("Choose a source to scrape data from:")
        print("1. Scrape from CVE website")
        print("2. Scrape from NVD API")
        print("3. Scrape from stackoverflow API")
        choice = input("Choose an option: ")
        if choice == "1":
            self.scrape_cve()
        elif choice == "2":
            self.scrape_nvd()
        elif choice == "3":
            self.scrapeStackOverflow()
        else:
            print("Invalid choice. Please try again.")
    
    def scrape_cve(self, limit=200):
        # Prepare a folder to store the jsons
        if not os.path.exists("cve_vulns"):
            os.mkdir("cve_vulns")

        try:
            # Initialize the Chrome WebDriver
            driver = webdriver.Chrome()  # specify the path to chromedriver

            # Navigate to the CVE website
            driver.get("https://www.exploit-db.com/")

            # Wait for the page to load
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "exploits-table")))

            sleep(1)
            # Find the table element containing vulnerability information
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            main_table = soup.find("table", {"id": "exploits-table"})
            exploits_table = main_table.find("tbody")

            # Let's get the number of total exploits in the page
            total_exploits = soup.find("div", {"class": "dataTables_info"})
            total_exploits = re.findall(r'Showing \d+ to \d+ of (.*) entries', total_exploits.text)[0]
            # Let's convert the number of exploits to an integer, but it has commas so let's replace them
            total_exploits = int(total_exploits.replace(",", ""))

            exploits_scraped = 0
            # Let's open the first exploit in the table and extract its link
            exploit = exploits_table.find("tr")
            row = exploit.find_all("td")

            # Let's get the link to the exploit and follow it
            link_for_vuln = "https://www.exploit-db.com" + row[4].find("a")["href"]
            # The driver can change to the new tab since we won't come back to the main page to scrape the rest of the
            # data
            while exploits_scraped < total_exploits and exploits_scraped < limit:
                # TODO: Verify if the exploit name is already in the database if so stop scraping

                driver.get(link_for_vuln)
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "code")))
                soup = BeautifulSoup(driver.page_source, 'html.parser')

                # We now have everything we need to scrape the exploit
                sleep(1)
                source = link_for_vuln
                vuln_name = soup.find("h1").text.strip()
                stats = soup.find_all("div", {"class": "card-stats"})
                # The card stats[0] contains data relative to the exploit id
                exploit_data = stats[0].find_all("h6")
                vuln_edbid = exploit_data[0].text.strip()
                vuln_cve = exploit_data[1].text.strip()

                # The card stats[1] contains data relative to the author name and the type of exploit
                exploit_data = stats[1].find_all("h6")
                author = exploit_data[0].text.strip()
                exploit_type = exploit_data[1].text.strip()

                # The card stats[2] contains data relative to the platform and the date the exploit was published
                exploit_data = stats[2].find_all("h6")
                platform = exploit_data[0].text.strip()
                date_published = exploit_data[1].text.strip()

                vuln_content = soup.find("code").text.strip()

                # embedding = self.generate_embedding(vulnContent)
                # Let's save the exploit to a file
                payload = {
                    "source": source,
                    "vulnName": vuln_name,
                    "vulnEDBID": vuln_edbid,
                    "vulnCVE": vuln_cve,
                    "author": author,
                    "exploitType": exploit_type,
                    "platform": platform,
                    "datePublished": date_published,
                    "vulnContent": vuln_content
                }

                # data = [payload]
                # Let's insert the new exploit into the database
                # self.db.insert_vulnerability(data)

                name_to_save = re.sub(r'[<>:"/\\|?*]', '_', vuln_name)
                name_to_save = name_to_save[:255]
                with open(f"cve_vulns/{name_to_save}.json", "w") as f:
                    f.write(json.dumps(payload))
                    print(f"Exploit {vuln_name} saved to file")

                # Go to the previous exploit page
                # Let's get the link to the exploit by locating the previous arrow button
                prev_button_div = soup.find("div", {"class": "ml-3"})
                prev_button = prev_button_div.find("a")
                link = prev_button["href"]
                link_for_vuln = "https://www.exploit-db.com" + link
                exploits_scraped += 1

        except Exception as e:
            print(e)
            print("An error occurred while scraping:", str(e))


    def scrape_nvd(self):
        # API_KEY = os.getenv("NVD_API_KEY")

        # Prepare a folder to store the jsons
        if not os.path.exists("nvd_vulns"):
            os.mkdir("nvd_vulns")


        # NVD API endpoint for retrieving CVE details
        startIndex = 0
        resultsPerPage = 1
        maxResults = 1
        nvd_api_url = f"https://services.nvd.nist.gov/rest/json/cves/2.0/?resultsPerPage={resultsPerPage}&startIndex={startIndex}"
        #Lets get the number of total exploits in the page
        response = req.get(nvd_api_url)
        data = response.json()
        totalExploits = data["totalResults"]
        #Set to 50 since is the maximum number of results per page the API allows for each 5 seconds 
        resultsPerPage = 50

        print(totalExploits)
        for _ in range(totalExploits // resultsPerPage):
            try:
                # Send a request to the NVD API
                response = req.get(nvd_api_url)
                data = response.json()
                # Check if the response contains CVE details
                if response.status_code == 200:
                    #Lets extract the vulnerabilities from the data
                    vulnerabilities = data["vulnerabilities"]
                    for vuln in vulnerabilities:
                        payload = {
                            "CVE_ID" : vuln["cve"]["id"],
                            "source" : vuln["cve"]["sourceIdentifier"],
                            "lastModifiedDate" : vuln["cve"]["lastModifiedDate"],
                        }


                # Update the start index for the next request
                startIndex += resultsPerPage
                nvd_api_url = f"https://services.nvd.nist.gov/rest/json/cves/2.0/?resultsPerPage={resultsPerPage}&startIndex={startIndex}"
            except Exception as e:
                print("An error occurred while scraping:", str(e))
                break


    def scrapeStackOverflow(self, tags=None):
        if not os.path.exists("stackoverflow_qna"):
            os.mkdir("stackoverflow_qna")

        pageNum = 1
        pageSize = 100
        authKey = "U4DMV*8nvpm3EOpvf69Rxw(("
        apiEndpoint = f"https://api.stackexchange.com/2.3/"
        stackFilter = "!51HTsOQRBj9p0_DazD)W)-DUaqtDn88)Rxjmvq"

        for _ in range(5):
            try:
               #Send request to the api
                link = f"{apiEndpoint}questions?page={pageNum}&pagesize={pageSize}&order=desc&sort=activity&key={authKey}&filter={stackFilter}&site=stackoverflow"
                response = req.get(link)
                if response.status_code==200:
                    
                    data = response.json()
                    #Lets extract the questions from the data
                    questions = data["items"]
                    for q in questions:
                        if q["is_answered"]:

                            questionTitle = q["title"]
                            questionBody = q["body"]
                            
                            #Lets try and get the accepted answer
                            try:
                                if q["is_accepted"]:
                                    
                                    acceptedAnswerId = q["accepted_answer_id"]
                                    for answer in q["answers"]:
                                        if answer["answer_id"] == acceptedAnswerId:
                                            acceptedAnswer = answer["body"]

                            except Exception as e:
                                #Lets get the one with the most upvotes if there is no answer then we will skip this question
                                acceptedAnswer = None
                                bestScore = 0
                                responses = q["answers"]
                                for r in responses:
                                    if r["score"] >= bestScore:
                                        acceptedAnswer = r["body"]
                                        bestScore = r["score"]
                            
                            if acceptedAnswer is not None:
                                payload = {
                                    "questionTitle": questionTitle,
                                    "question": questionBody,
                                    "acceptedAnswer": acceptedAnswer,
                                    "questionSource": q["link"]
                                }
                                #Lets insert into a json file
                                questionID = q["question_id"]
                                with open(f"stackoverflow_qna/stackoverflow_{questionID}.json", "w") as f:
                                    json.dump(payload, f)
                            
                pageNum+=1

            except Exception as e:
                print("An error occurred while scraping:", str(e))
                break
        
        print("Finished scraping stackoverflow")

    
                    
DataScraper()
